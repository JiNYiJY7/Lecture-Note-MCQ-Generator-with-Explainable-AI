from __future__ import annotations
import re
from typing import Any, List, Optional
from litellm import completion

from app.core.database import SessionLocal
from app.core.llm_client import call_deepseek_chat
from app.modules.xai import schemas as xai_schemas
from app.modules.mcq_management import models as mcq_models

OFFLINE_MODEL = "ollama/llama3.2:1b"


def _normalize_choice_label(x: Any) -> str:
    s = str(x or "").strip()
    if not s: return ""
    m = re.search(r"\b([A-D])\b", s, flags=re.IGNORECASE)
    if m: return m.group(1).upper()
    return s[:1].upper()


def _generate_ai_explanation(
        lecture_text: str, question_stem: str, student_text: str,
        correct_text: str, is_correct: bool, use_offline: bool = False
) -> str:
    """Generate explanation using DeepSeek (Online) or Llama (Offline)."""

    verdict = "CORRECT" if is_correct else "INCORRECT"
    prompt = f"""
    CONTEXT: {lecture_text[:1500]}
    QUESTION: {question_stem}
    STUDENT ANSWER: {student_text}
    REAL ANSWER: {correct_text}
    VERDICT: {verdict}

    TASK: Explain WHY it is {verdict}. 
    FORMAT: Start with "{verdict.title()} - ". Keep it under 2 sentences.
    """

    try:
        if use_offline:
            print(f"   üîå Tool generating with {OFFLINE_MODEL}...")
            response = completion(
                model=OFFLINE_MODEL,
                messages=[{"role": "user", "content": prompt}],
                api_base="http://localhost:11434",
                timeout=60
            )

            content = response.choices[0].message.content
            if not content or not content.strip():
                print("   ‚ö†Ô∏è Local LLM returned empty response.")
                return f"{verdict.title()} - (No explanation generated by local model)."

            return content.strip().replace('"', '')

        else:
            return call_deepseek_chat("You are a tutor.", prompt).strip().replace('"', '')

    except Exception as e:
        print(f"   ‚ùå Tool generation failed: {e}")
        return f"{verdict.title()} - (Explanation unavailable due to error: {e})."


def explain_mcq_answer_tool(
        question_id: int, student_answer_label: str,
        use_offline: bool = False,
        **kwargs
) -> str:
    """Checks answer, checks cache, generates explanation, and saves to DB."""
    db = SessionLocal()
    try:
        # 1. Fetch Question
        from app.modules.mcq_management import service as mcq_service
        q = mcq_service.get_question_by_id(db, question_id)

        if not q:
            return "Error: Question not found."

        # 2. Check Logic & Identify Option IDs
        student_label = _normalize_choice_label(student_answer_label)
        correct_label = _normalize_choice_label(q.answer_key.correct_option.label)
        is_correct = (student_label == correct_label)

        student_text = ""
        correct_text = ""
        selected_option_id = None

        # Loop to find texts and the specific Option ID for the student's choice
        for o in q.options:
            norm_label = _normalize_choice_label(o.label)
            if norm_label == student_label:
                student_text = o.text
                selected_option_id = o.id
            if norm_label == correct_label:
                correct_text = o.text

        # ---------------------------------------------------------
        # 3. CHECK CACHE (Logic from your old code)
        # ---------------------------------------------------------
        if selected_option_id:
            existing_explanation = (
                db.query(mcq_models.Explanation)
                .filter(
                    mcq_models.Explanation.question_id == question_id,
                    mcq_models.Explanation.option_id == selected_option_id,
                    mcq_models.Explanation.source == "ai_generated"
                )
                .first()
            )

            if existing_explanation:
                print(f"   ‚ö° CACHE HIT: Found saved explanation for Option {student_label}")
                return existing_explanation.content

        # 4. Generate New Explanation
        ai_explanation = _generate_ai_explanation(
            lecture_text=q.lecture.clean_text,
            question_stem=q.stem,
            student_text=student_text,
            correct_text=correct_text,
            is_correct=is_correct,
            use_offline=use_offline
        )

        # ---------------------------------------------------------
        # 5. SAVE TO CACHE (Logic from your old code)
        # ---------------------------------------------------------
        if selected_option_id:
            try:
                new_expl = mcq_models.Explanation(
                    question_id=question_id,
                    option_id=selected_option_id,
                    content=ai_explanation,
                    source="ai_generated",
                )
                db.add(new_expl)
                db.commit()
                print(f"   üíæ SAVED AI-generated explanation for Option {student_label}.")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Warning: Failed to save explanation: {e}")
                db.rollback()

        return ai_explanation

    except Exception as e:
        print(f"   ‚ö†Ô∏è Agent Tool Error: {e}")
        return f"Error: {str(e)}"
    finally:
        db.close()